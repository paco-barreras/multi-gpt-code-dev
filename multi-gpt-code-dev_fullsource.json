[
  {
    "file_path": "context_store.py",
    "element_name": "_get_ast_node_source_segment",
    "element_type": "FunctionDef",
    "start_line": 44,
    "end_line": 65,
    "docstring": "",
    "source_code": "def _get_ast_node_source_segment(source_lines, node):\n    if not (hasattr(node, 'lineno') and hasattr(node, 'end_lineno')):\n        return None\n    start_line_idx = node.lineno - 1\n    end_line_idx = node.end_lineno\n    if hasattr(node, 'decorator_list') and node.decorator_list:\n        first_decorator = node.decorator_list[0]\n        if hasattr(first_decorator, 'lineno'):\n            decorator_start_line_idx = first_decorator.lineno - 1\n            if decorator_start_line_idx < start_line_idx:\n                start_line_idx = decorator_start_line_idx\n    if start_line_idx < 0 or end_line_idx > len(source_lines):\n        try: return ast.unparse(node) # Fallback for Python 3.9+\n        except: return f\"# Error: Could not reliably get source for {getattr(node, 'name', 'unknown_node')}\"\n    segment_lines = source_lines[start_line_idx:end_line_idx]\n    actual_def_line_in_segment_idx = (node.lineno - 1) - start_line_idx\n    if 0 <= actual_def_line_in_segment_idx < len(segment_lines):\n        first_def_line = segment_lines[actual_def_line_in_segment_idx]\n        indentation = len(first_def_line) - len(first_def_line.lstrip())\n        dedented_lines = [line[indentation:] if line.startswith(' ' * indentation) else line for line in segment_lines]\n        return \"\".join(dedented_lines)\n    return \"\".join(segment_lines)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_extract_ast_chunks_from_file",
    "element_type": "FunctionDef",
    "start_line": 67,
    "end_line": 84,
    "docstring": "",
    "source_code": "def _extract_ast_chunks_from_file(py_file_path, repo_root_path):\n    try:\n        file_content = py_file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n        source_lines = file_content.splitlines(True)\n        tree = ast.parse(file_content, filename=str(py_file_path))\n    except Exception as e:\n        return\n    file_rel_path_str = str(py_file_path.relative_to(repo_root_path))\n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            source_code_snippet = _get_ast_node_source_segment(source_lines, node)\n            if source_code_snippet:\n                yield {\n                    \"file_path\": file_rel_path_str, \"element_name\": node.name,\n                    \"element_type\": node.__class__.__name__, \"start_line\": node.lineno,\n                    \"end_line\": node.end_lineno, \"docstring\": ast.get_docstring(node) or \"\",\n                    \"source_code\": source_code_snippet\n                }\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_get_sentence_transformer_model",
    "element_type": "FunctionDef",
    "start_line": 86,
    "end_line": 92,
    "docstring": "",
    "source_code": "def _get_sentence_transformer_model(model_name):\n    if model_name not in _CACHED_MODELS:\n        from sentence_transformers import SentenceTransformer\n        print(f\"Info: Loading SentenceTransformer model: {model_name}...\", file=sys.stderr)\n        _CACHED_MODELS[model_name] = SentenceTransformer(model_name, device=\"cpu\")\n        print(f\"Info: Model {model_name} loaded.\", file=sys.stderr)\n    return _CACHED_MODELS[model_name]\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_embed_texts_batch",
    "element_type": "FunctionDef",
    "start_line": 94,
    "end_line": 100,
    "docstring": "",
    "source_code": "def _embed_texts_batch(texts, model_name, is_query=False):\n    if not texts: return np.array([])\n    texts_to_embed = [f\"query: {text}\" for text in texts] if is_query else texts\n    show_progress = not is_query\n    model = _get_sentence_transformer_model(model_name)\n    return model.encode(texts_to_embed, batch_size=32 if not is_query else 1,\n                        show_progress_bar=show_progress, normalize_embeddings=True)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "build_index",
    "element_type": "FunctionDef",
    "start_line": 102,
    "end_line": 124,
    "docstring": "",
    "source_code": "def build_index(repo_root_path, index_output_path, model_name=DEFAULT_MODEL):\n    repo_root, index_file = Path(repo_root_path).resolve(), Path(index_output_path).resolve()\n    if not repo_root.is_dir(): raise FileNotFoundError(f\"Repo root not found: {repo_root}\")\n    all_src_texts, all_meta = [], []\n    print(f\"Info: Scanning Python files in: {repo_root} for AST chunking...\", file=sys.stderr)\n    py_files = [p for p in repo_root.rglob(\"*.py\") if not any(ex in p.parts for ex in\n                ['.git', '.vscode', '.idea', '__pycache__', 'node_modules', 'build', 'dist',\n                 'venv', 'env', '.env', 'site-packages', '.ipynb_checkpoints', 'tests', 'test', 'docs/_build'])]\n    print(f\"Info: Found {len(py_files)} Python files to process.\", file=sys.stderr)\n    for py_path in py_files:\n        for chunk_dict in _extract_ast_chunks_from_file(py_path, repo_root):\n            all_src_texts.append(chunk_dict[\"source_code\"])\n            all_meta.append(chunk_dict)\n    if not all_src_texts:\n        print(\"Warning: No AST chunks found to index. Creating an empty index.\", file=sys.stderr)\n        index_file.parent.mkdir(parents=True, exist_ok=True)\n        np.savez_compressed(index_file, embeddings=np.array([]), meta=np.array([], dtype=object))\n        print(f\"Info: Empty index written to {index_file}\", file=sys.stderr)\n        return\n    embeddings = _embed_texts_batch(all_src_texts, model_name, is_query=False)\n    index_file.parent.mkdir(parents=True, exist_ok=True)\n    np.savez_compressed(index_file, embeddings=embeddings, meta=np.array(all_meta, dtype=object))\n    print(f\"âœ“ Index with {len(all_meta)} AST chunks written to {index_file}\", file=sys.stderr)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_load_index_from_file",
    "element_type": "FunctionDef",
    "start_line": 126,
    "end_line": 131,
    "docstring": "",
    "source_code": "def _load_index_from_file(index_file_path):\n    data = np.load(index_file_path, allow_pickle=True)\n    embeds_np, meta_np = data.get(\"embeddings\"), data.get(\"meta\")\n    if embeds_np is None or meta_np is None: raise ValueError(f\"Index missing 'embeddings' or 'meta': {index_file_path}\")\n    meta_list = [dict(item) for item in meta_np]\n    return torch.tensor(embeds_np, dtype=torch.float32), meta_list\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "get_code_context",
    "element_type": "FunctionDef",
    "start_line": 133,
    "end_line": 165,
    "docstring": "",
    "source_code": "def get_code_context(query, index_file_path, k=3, max_tokens=2000, query_model_name=DEFAULT_MODEL):\n    idx_path = Path(index_file_path).resolve()\n    if idx_path not in _CACHED_INDICES:\n        if not idx_path.exists(): raise FileNotFoundError(f\"Index file not found: {idx_path}\")\n        _CACHED_INDICES[idx_path] = _load_index_from_file(idx_path)\n    embeds_tensor, meta_list = _CACHED_INDICES[idx_path]\n    if embeds_tensor.nelement() == 0: return []\n    q_embed_np = _embed_texts_batch([query], query_model_name, is_query=True)[0]\n    q_tensor = torch.tensor(q_embed_np, dtype=torch.float32).to(embeds_tensor.device)\n    if embeds_tensor.ndim == 1: embeds_tensor = embeds_tensor.unsqueeze(0)\n    if q_tensor.ndim > 1: q_tensor = q_tensor.squeeze()\n    if embeds_tensor.shape[0] == 0 or embeds_tensor.shape[1] != q_tensor.shape[0]:\n        return []\n    sims = (embeds_tensor @ q_tensor).cpu().numpy()\n    actual_k = min(k, len(sims))\n    if actual_k == 0: return []\n    top_indices = sims.argsort()[-actual_k:][::-1]\n    results, current_tokens = [], 0\n    for hit_idx in top_indices:\n        chunk_meta = meta_list[hit_idx]\n        snippet_text = chunk_meta[\"source_code\"]\n        token_count = len(snippet_text.split())\n        if current_tokens + token_count > max_tokens and len(results) > 0:\n            if k == 1 and not results: pass\n            else: continue\n        results.append({\n            \"file\": chunk_meta[\"file_path\"], \"lines\": f\"{chunk_meta['start_line']}-{chunk_meta['end_line']}\",\n            \"snippet\": snippet_text, \"element_name\": chunk_meta[\"element_name\"],\n            \"element_type\": chunk_meta[\"element_type\"], \"docstring\": chunk_meta[\"docstring\"]\n        })\n        current_tokens += token_count\n        if len(results) >= k: break\n    return results\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_handle_build_cli",
    "element_type": "FunctionDef",
    "start_line": 167,
    "end_line": 168,
    "docstring": "",
    "source_code": "def _handle_build_cli(args):\n    build_index(repo_root_path=args.repo, index_output_path=args.index, model_name=args.model)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_handle_query_cli",
    "element_type": "FunctionDef",
    "start_line": 170,
    "end_line": 186,
    "docstring": "",
    "source_code": "def _handle_query_cli(args):\n    try:\n        results = get_code_context(query=args.query, index_file_path=args.index, k=args.k,\n                                   max_tokens=args.max_tokens, query_model_name=args.model)\n        if results:\n            print(\"=== Query Results ===\")\n            for res_idx, res in enumerate(results):\n                print(f\"\\n--- Result {res_idx+1} ---\")\n                print(f\"File: {res['file']}\")\n                print(f\"Element: {res['element_name']} ({res['element_type']})\")\n                print(f\"Lines: {res['lines']}\")\n                if res.get('docstring'): print(f\"Docstring: {res['docstring'][:200]}{'...' if len(res['docstring']) > 200 else ''}\")\n                print(f\"Snippet:\\n{res['snippet']}\")\n        else:\n            print(\"No relevant snippets found.\")\n    except FileNotFoundError as e: print(f\"Error: {e}. Ensure index file exists.\", file=sys.stderr)\n    except Exception as e: print(f\"An error occurred during query: {e}\", file=sys.stderr)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "process_source",
    "element_type": "FunctionDef",
    "start_line": 188,
    "end_line": 237,
    "docstring": "",
    "source_code": "def process_source(path, text, elem_type, chunks, meta, repo):\n    lines = text.splitlines(True)\n    if not lines:\n        return\n\n    active_headings_stack = []\n    discovered_headings_info = []\n    inside_code_block = False\n\n    for i, line_content in enumerate(lines):\n        stripped_line = line_content.lstrip()\n        # Toggle fenced code block state and skip the fence lines\n        if stripped_line.startswith(\"```\"):\n            inside_code_block = not inside_code_block\n            continue\n        # Skip heading detection inside fenced code blocks\n        if inside_code_block:\n            continue\n        # Detect Markdown headings outside code blocks\n        if stripped_line.startswith(\"#\"):\n            heading_level = len(stripped_line) - len(stripped_line.lstrip(\"#\"))\n            heading_title = stripped_line[heading_level:].strip()\n\n            while active_headings_stack and active_headings_stack[-1][0] >= heading_level:\n                active_headings_stack.pop()\n            active_headings_stack.append((heading_level, heading_title))\n            discovered_headings_info.append((i, heading_level, list(active_headings_stack)))\n\n    # Sentinel to mark end of last section\n    discovered_headings_info.append((len(lines), 0, []))\n\n    for idx in range(len(discovered_headings_info) - 1):\n        current_start, _, current_path = discovered_headings_info[idx]\n        next_start, _, _ = discovered_headings_info[idx + 1]\n\n        snippet_lines = lines[current_start:next_start]\n        if sum(1 for l in snippet_lines if l.strip()) < 5:\n            continue\n\n        snippet_text = ''.join(snippet_lines)\n        heading_path_str = \" > \".join(title for _, title in current_path)\n\n        meta.append({\n            \"file_path\": str(path.relative_to(repo)),\n            \"heading_path\": heading_path_str,\n            \"element_type\": elem_type,\n            \"start_line\": current_start + 1,\n            \"end_line\": next_start,\n        })\n        chunks.append(snippet_text)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "build_prose_index",
    "element_type": "FunctionDef",
    "start_line": 239,
    "end_line": 322,
    "docstring": "",
    "source_code": "def build_prose_index(repo_root_path, index_output_path, model_name=DEFAULT_MODEL):\n    repo_root_path = Path(repo_root_path).resolve()\n    index_output_path = Path(index_output_path)\n\n    all_chunks_text = []\n    all_chunks_meta = []\n\n    prose_extensions = {\".md\", \".txt\", \".rst\"}\n    notebook_extensions = {\".ipynb\"}\n\n    EXCLUDE_DIR_NAMES_EXACT = {\n        '.ipynb_checkpoints', 'build', 'dist', '__pycache__', \n        'venv', 'node_modules'\n    }\n\n    for file_path in repo_root_path.rglob(\"*\"):\n        should_skip = False\n        try:\n            relative_parent_dirs = file_path.relative_to(repo_root_path).parts[:-1]\n            for part_name in relative_parent_dirs:\n                if part_name.startswith('.') or part_name in EXCLUDE_DIR_NAMES_EXACT:\n                    should_skip = True\n                    break\n        except ValueError:\n            should_skip = True\n        \n        if should_skip:\n            continue\n\n        if not file_path.is_file():\n            continue\n\n        if file_path.suffix in prose_extensions:\n            try:\n                text_content = file_path.read_text(encoding=\"utf-8\")\n                elem_type = \"Markdown\" if file_path.suffix == \".md\" else \"ProseText\"\n                process_source(file_path, text_content, elem_type, \n                               all_chunks_text, all_chunks_meta, repo_root_path)\n            except Exception as e:\n                print(f\"Error processing prose file {file_path}: {e}\", file=sys.stderr)\n\n        elif file_path.suffix in notebook_extensions:\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    notebook = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n\n                markdown_cell_sources = []\n                for cell in notebook.cells:\n                    if \"ignore\" in cell.metadata.get(\"tags\", []):\n                        continue\n\n                    if cell.cell_type == \"markdown\":\n                        markdown_cell_sources.append(cell.source)\n                    elif cell.cell_type == \"code\":\n                        cell_source_stripped = cell.source.strip()\n                        if cell_source_stripped.startswith((\"!\", \"%\")):\n                            continue\n                \n                if markdown_cell_sources:\n                    concatenated_markdown_content = \"\\n\\n\".join(markdown_cell_sources)\n                    process_source(file_path, concatenated_markdown_content, \"Notebook\",\n                                   all_chunks_text, all_chunks_meta, repo_root_path)\n            except Exception as e:\n                print(f\"Error processing notebook {file_path}: {e}\", file=sys.stderr)\n    \n    if not all_chunks_text:\n        print(f\"No text chunks found in {repo_root_path} after exclusions. Prose index will not be built.\", file=sys.stderr)\n        return\n\n    try:\n        embeddings_array = _embed_texts_batch(all_chunks_text, model_name, is_query=False) \n\n        index_output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        np.savez_compressed(\n            index_output_path,\n            embeddings=embeddings_array,\n            texts=np.array(all_chunks_text, dtype=object), \n            metadata=all_chunks_meta \n        )\n        print(f\"Prose index built with {len(all_chunks_text)} chunks and saved to {index_output_path}\", file=sys.stderr)\n\n    except Exception as e:\n        print(f\"Failed to build or save prose index for {repo_root_path}: {e}\", file=sys.stderr)\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "get_prose_context",
    "element_type": "FunctionDef",
    "start_line": 324,
    "end_line": 376,
    "docstring": "",
    "source_code": "def get_prose_context(query, index_file_path, k=3, model_name=DEFAULT_MODEL):\n    try:\n        data = np.load(str(index_file_path), allow_pickle=True)\n        embeddings = data[\"embeddings\"]\n        texts = data[\"texts\"]\n        meta = list(data[\"metadata\"])\n    except FileNotFoundError:\n        print(f\"Error: Index file not found at {index_file_path}\", file=sys.stderr)\n        return []\n    except KeyError as e:\n        print(f\"Error: Index file {index_file_path} is missing expected field: {e}\", file=sys.stderr)\n        return []\n\n    if embeddings.size == 0 or len(texts) == 0:\n        print(f\"Warning: Index {index_file_path} contains no data.\", file=sys.stderr)\n        return []\n        \n    query_embedding = _embed_texts_batch([query], model_name, is_query=True)\n    if query_embedding.ndim > 1:\n        query_embedding = query_embedding[0]\n\n    q_norm = np.linalg.norm(query_embedding)\n    e_norm = np.linalg.norm(embeddings, axis=1)\n\n    if q_norm == 0:\n        similarities = np.zeros(embeddings.shape[0])\n    else:\n        valid_e_norm_mask = e_norm > 0\n        similarities = np.zeros(embeddings.shape[0])\n        if np.any(valid_e_norm_mask):\n            dot_product = np.dot(embeddings[valid_e_norm_mask], query_embedding)\n            similarities[valid_e_norm_mask] = dot_product / (e_norm[valid_e_norm_mask] * q_norm)\n        \n    num_items = len(similarities)\n    actual_k = min(k, num_items)\n    if actual_k == 0 and num_items > 0:\n        return []\n    elif num_items == 0:\n        return []\n\n    ids = np.argsort(similarities)[::-1][:actual_k].astype(int)\n\n    results = []\n    for i in ids:\n        m = meta[i]\n        results.append({\n            \"file\": m[\"file_path\"],\n            \"heading_path\": m[\"heading_path\"],\n            \"element_type\": m[\"element_type\"],\n            \"lines\": f\"{m['start_line']}-{m['end_line']}\",\n            \"snippet\": texts[i],\n        })\n    return results\n"
  },
  {
    "file_path": "context_store.py",
    "element_name": "_cli_main",
    "element_type": "FunctionDef",
    "start_line": 378,
    "end_line": 449,
    "docstring": "",
    "source_code": "def _cli_main(argv=None):\n    if argv is None: argv = sys.argv[1:]\n    parser = argparse.ArgumentParser(description=\"Build or query AST-based dense code index.\",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Sub-command to execute\", required=True if argv else False)\n    \n    _json_build = subparsers.add_parser(\"build-json\",\n                                        help=\"Export AST chunks to JSON context files\")\n    _json_build.add_argument(\"--repo\", required=True,\n                             help=\"Path to repository root\")\n    _json_build.add_argument(\"--output-base-name\", required=True,\n                             help=\"Base name for output JSON files\")\n    _json_build.set_defaults(\n        func=lambda a: export_ast_chunks_to_json(a.repo, a.output_base_name))\n    \n    _json_query = subparsers.add_parser(\"query-json\",\n                                        help=\"Query a JSON context store\")\n    _json_query.add_argument(\"--signatures-file\", required=True,\n                             help=\"Path to signatures JSON\")\n    _json_query.add_argument(\"--source-file\", required=True,\n                             help=\"Path to fullsource JSON\")\n    _json_query.add_argument(\"--query\", required=True,\n                             help=\"Search query string\")\n    _json_query.add_argument(\"--k\", type=int, default=3,\n                             help=\"Number of results to return\")\n    _json_query.set_defaults(\n        func=lambda a: print(json.dumps(\n            query_json_context(a.query,\n                               a.signatures_file,\n                               a.source_file,\n                               a.k),\n            ensure_ascii=False, indent=2)))\n\n    _pb = subparsers.add_parser(\"build-prose\", help=\"Build prose embedding index\")\n    _pb.add_argument(\"--repo\", required=True, help=\"Path to repo root\")\n    _pb.add_argument(\"--output\", required=True, help=\"Output base path for prose index\")\n    _pb.add_argument(\"--model\", default=DEFAULT_MODEL, help=\"Embedding model name\")\n    _pb.set_defaults(func=lambda args: build_prose_index(args.repo, args.output, args.model))\n\n    _pq = subparsers.add_parser(\"query-prose\", help=\"Query prose embedding index\")\n    _pq.add_argument(\"--index\", required=True, help=\"Path to prose index (.npz)\")\n    _pq.add_argument(\"--query\", required=True, help=\"Query text\")\n    _pq.add_argument(\"--k\", type=int, default=3, help=\"Number of results\")\n    _pq.add_argument(\"--model\", default=DEFAULT_MODEL, help=\"Embedding model name\")\n    _pq.set_defaults(func=lambda args: print(\n        json.dumps(\n            get_prose_context(\n                query=args.query,\n                index_file_path=args.index,\n                k=args.k,\n                model_name=args.model\n            ),\n            ensure_ascii=False, indent=2\n        )\n    ))\n    # Build\n    p_build = subparsers.add_parser(\"build\", help=\"Build dense code index.\")\n    p_build.add_argument(\"--repo\", type=str, required=True, help=\"Path to code repository root.\")\n    p_build.add_argument(\"--index\", type=str, required=True, help=\"Path to save output .npz index file.\")\n    p_build.add_argument(\"--model\", type=str, default=DEFAULT_MODEL, help=\"SentenceTransformer model name.\")\n    p_build.set_defaults(func=_handle_build_cli)\n    # Query\n    p_query = subparsers.add_parser(\"query\", help=\"Query dense code index.\")\n    p_query.add_argument(\"--index\", type=str, required=True, help=\"Path to .npz index file.\")\n    p_query.add_argument(\"--query\", type=str, required=True, help=\"Natural language query string.\")\n    p_query.add_argument(\"--k\", type=int, default=3, help=\"Number of top results.\")\n    p_query.add_argument(\"--model\", type=str, default=DEFAULT_MODEL, help=\"SentenceTransformer model for query.\")\n    p_query.set_defaults(func=_handle_query_cli)\n\n    if not argv: parser.print_help(sys.stderr); sys.exit(1)\n    args = parser.parse_args(argv)\n    args.func(args)\n"
  },
  {
    "file_path": "context_store_json.py",
    "element_name": "_get_ast_node_source_segment",
    "element_type": "FunctionDef",
    "start_line": 8,
    "end_line": 54,
    "docstring": "\n    Extracts the source code segment for an AST node, including decorators.\n    Tries to dedent the segment relative to its actual definition line.\n\n    Args:\n        source_lines (list[str]): A list of strings, where each string is a line of source code.\n        node (ast.AST): The AST node for which to extract the source segment.\n\n    Returns:\n        str: The extracted source code segment as a string.\n    ",
    "source_code": "def _get_ast_node_source_segment(source_lines, node):\n    \"\"\"\n    Extracts the source code segment for an AST node, including decorators.\n    Tries to dedent the segment relative to its actual definition line.\n\n    Args:\n        source_lines (list[str]): A list of strings, where each string is a line of source code.\n        node (ast.AST): The AST node for which to extract the source segment.\n\n    Returns:\n        str: The extracted source code segment as a string.\n    \"\"\"\n    if not (hasattr(node, 'lineno') and hasattr(node, 'end_lineno')):\n        try: return ast.unparse(node)\n        except: return f\"# Error: Could not unparse {getattr(node, 'name', 'unknown_node')}\"\n\n    start_line_idx = node.lineno - 1\n    end_line_idx = node.end_lineno\n\n    if hasattr(node, 'decorator_list') and node.decorator_list:\n        first_decorator = node.decorator_list[0]\n        if hasattr(first_decorator, 'lineno'):\n            decorator_start_line_idx = first_decorator.lineno - 1\n            if decorator_start_line_idx < start_line_idx:\n                start_line_idx = decorator_start_line_idx\n    \n    if start_line_idx < 0 or end_line_idx > len(source_lines):\n        try: return ast.unparse(node)\n        except: return f\"# Error: Could not reliably get source for {getattr(node, 'name', 'unknown_node')}\"\n\n    segment_lines = source_lines[start_line_idx:end_line_idx]\n    \n    actual_def_line_in_segment_idx = (node.lineno - 1) - start_line_idx\n    if 0 <= actual_def_line_in_segment_idx < len(segment_lines):\n        first_def_line = segment_lines[actual_def_line_in_segment_idx]\n        indentation = len(first_def_line) - len(first_def_line.lstrip())\n        if indentation > 0:\n            can_dedent = True\n            for line_to_check in segment_lines:\n                if line_to_check.strip() and not line_to_check.startswith(' ' * indentation):\n                    can_dedent = False\n                    break\n            if can_dedent:\n                dedented_lines = [line[indentation:] for line in segment_lines]\n                return \"\".join(dedented_lines)\n                \n    return \"\".join(segment_lines)\n"
  },
  {
    "file_path": "context_store_json.py",
    "element_name": "_extract_ast_chunks_from_file",
    "element_type": "FunctionDef",
    "start_line": 57,
    "end_line": 109,
    "docstring": "\n    Parses a Python file and yields comprehensive dictionaries for functions and classes.\n    Each dictionary includes metadata, docstring, full source_code, and a multi-line signature.\n\n    Args:\n        py_file_path (pathlib.Path): Path to the Python file to parse.\n        repo_root_path (pathlib.Path): Path to the root of the repository for relative path calculation.\n\n    Yields:\n        Iterator[dict[str, any]]: An iterator of dictionaries, each representing an AST chunk.\n    ",
    "source_code": "def _extract_ast_chunks_from_file(py_file_path, repo_root_path):\n    \"\"\"\n    Parses a Python file and yields comprehensive dictionaries for functions and classes.\n    Each dictionary includes metadata, docstring, full source_code, and a multi-line signature.\n\n    Args:\n        py_file_path (pathlib.Path): Path to the Python file to parse.\n        repo_root_path (pathlib.Path): Path to the root of the repository for relative path calculation.\n\n    Yields:\n        Iterator[dict[str, any]]: An iterator of dictionaries, each representing an AST chunk.\n    \"\"\"\n    try:\n        file_content = py_file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n        source_lines = file_content.splitlines(True)\n        tree = ast.parse(file_content, filename=str(py_file_path))\n    except Exception:\n        return\n    \n    rel_path_str = str(py_file_path.relative_to(repo_root_path))\n\n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            source_code_snippet = _get_ast_node_source_segment(source_lines, node)\n            if not source_code_snippet or source_code_snippet.startswith(\"# Error:\"):\n                continue\n\n            sig_lines_extracted = []\n            snippet_lines_for_sig = source_code_snippet.splitlines(True)\n            \n            for line in snippet_lines_for_sig:\n                stripped_line = line.lstrip()\n                if stripped_line.startswith(\"@\"):\n                    sig_lines_extracted.append(line)\n                elif stripped_line.startswith((\"def \", \"async def \", \"class \")):\n                    sig_lines_extracted.append(line)\n                    break\n            \n            signature_text = \"\"\n            if sig_lines_extracted:\n                first_sig_line = sig_lines_extracted[0].lstrip()\n                signature_text = first_sig_line + \"\".join(sig_lines_extracted[1:])\n\n            yield {\n                \"file_path\": rel_path_str,\n                \"element_name\": node.name,\n                \"element_type\": node.__class__.__name__,\n                \"start_line\": node.lineno,\n                \"end_line\": node.end_lineno,\n                \"docstring\": ast.get_docstring(node, clean=False) or \"\",\n                \"signature\": signature_text.strip(),\n                \"source_code\": source_code_snippet,\n            }\n"
  },
  {
    "file_path": "context_store_json.py",
    "element_name": "build_json_indices",
    "element_type": "FunctionDef",
    "start_line": 113,
    "end_line": 175,
    "docstring": "\n    Scans a Python repository, extracts AST chunks, and saves two JSON files:\n    - <repo_name>_signatures.json: Contains public (non-underscore-prefixed) elements' signatures and metadata.\n    - <repo_name>_fullsource.json: Contains all extracted elements' full source code and metadata.\n    Files are saved to the specified output directory.\n\n    Args:\n        repo_path_str (str | pathlib.Path): Path to the root directory of the Python repository.\n        output_dir_str (str | pathlib.Path): Directory to save the generated JSON index files.\n    ",
    "source_code": "def build_json_indices(repo_path_str, output_dir_str):\n    \"\"\"\n    Scans a Python repository, extracts AST chunks, and saves two JSON files:\n    - <repo_name>_signatures.json: Contains public (non-underscore-prefixed) elements' signatures and metadata.\n    - <repo_name>_fullsource.json: Contains all extracted elements' full source code and metadata.\n    Files are saved to the specified output directory.\n\n    Args:\n        repo_path_str (str | pathlib.Path): Path to the root directory of the Python repository.\n        output_dir_str (str | pathlib.Path): Directory to save the generated JSON index files.\n    \"\"\"\n    repo_path = Path(repo_path_str).resolve()\n    output_path = Path(output_dir_str).resolve()\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    repo_name = repo_path.name\n    signatures_list = []\n    fullsource_list = []\n\n    py_files = [p for p in repo_path.rglob(\"*.py\") if not any(ex in p.parts for ex in\n                ['.git', '.vscode', '.idea', '__pycache__', 'node_modules', 'build', 'dist',\n                 'venv', 'env', '.env', 'site-packages', '.ipynb_checkpoints'])]\n\n    for py_file in py_files:\n        try:\n            for chunk in _extract_ast_chunks_from_file(py_file, repo_path):\n                # Add to fullsource_list unconditionally\n                fullsource_list.append({\n                    \"file_path\": chunk[\"file_path\"],\n                    \"element_name\": chunk[\"element_name\"],\n                    \"element_type\": chunk[\"element_type\"],\n                    \"start_line\": chunk[\"start_line\"],\n                    \"end_line\": chunk[\"end_line\"],\n                    \"docstring\": chunk[\"docstring\"],\n                    \"source_code\": chunk[\"source_code\"],\n                })\n\n                # Add to signatures_list only if not an internal element\n                # (name doesn't start with '_' or is a dunder method which is usually public)\n                if not (chunk[\"element_name\"].startswith(\"_\") and \\\n                        not (chunk[\"element_name\"].startswith(\"__\") and chunk[\"element_name\"].endswith(\"__\"))):\n                    signatures_list.append({\n                        \"file_path\": chunk[\"file_path\"],\n                        \"element_name\": chunk[\"element_name\"],\n                        \"element_type\": chunk[\"element_type\"],\n                        \"start_line\": chunk[\"start_line\"],\n                        \"end_line\": chunk[\"end_line\"],\n                        \"docstring\": chunk[\"docstring\"],\n                        \"signature\": chunk[\"signature\"],\n                    })\n        except (SyntaxError, UnicodeDecodeError) as e:\n            print(f\"Warning: Skipping file {py_file} due to error: {e}\", file=sys.stderr)\n            continue\n        \n    sig_file_path = output_path / f\"{repo_name}_signatures.json\"\n    full_file_path = output_path / f\"{repo_name}_fullsource.json\"\n    \n    with open(sig_file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(signatures_list, f, ensure_ascii=False, indent=2)\n    with open(full_file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(fullsource_list, f, ensure_ascii=False, indent=2)\n        \n    print(f\"JSON indices exported to:\\n- {sig_file_path.resolve()}\\n- {full_file_path.resolve()}\", file=sys.stderr)\n"
  },
  {
    "file_path": "context_store_json.py",
    "element_name": "query_json_file",
    "element_type": "FunctionDef",
    "start_line": 178,
    "end_line": 237,
    "docstring": "\n    Queries a JSON index file (either signatures or full source) for relevant code elements.\n    Search is performed on 'element_name' and 'docstring'.\n    The structure of returned elements depends on the input index file.\n\n    Args:\n        query_str (str): Search query string.\n        index_file_path_str (str | pathlib.Path): Path to the JSON index file to query.\n        k (int, optional): Number of top results to return. Defaults to 3.\n\n    Returns:\n        list[dict[str, any]]: A list of dictionaries, each representing a matching code element.\n    ",
    "source_code": "def query_json_file(query_str, index_file_path_str, k=3):\n    \"\"\"\n    Queries a JSON index file (either signatures or full source) for relevant code elements.\n    Search is performed on 'element_name' and 'docstring'.\n    The structure of returned elements depends on the input index file.\n\n    Args:\n        query_str (str): Search query string.\n        index_file_path_str (str | pathlib.Path): Path to the JSON index file to query.\n        k (int, optional): Number of top results to return. Defaults to 3.\n\n    Returns:\n        list[dict[str, any]]: A list of dictionaries, each representing a matching code element.\n    \"\"\"\n    index_path = Path(index_file_path_str)\n    if not index_path.is_file():\n        print(f\"Error: Index file not found at {index_path}\", file=sys.stderr)\n        return []\n\n    with open(index_path, encoding=\"utf-8\") as f:\n        indexed_elements = json.load(f)\n\n    if not indexed_elements:\n        return []\n\n    query_tokens = {token.lower() for token in query_str.split() if token}\n    if not query_tokens:\n        return []\n\n    scored_matches = []\n    for element in indexed_elements:\n        name_to_search = element.get(\"element_name\", \"\").lower()\n        docstring_to_search = element.get(\"docstring\", \"\").lower()\n        search_text = f\"{name_to_search} {docstring_to_search}\"\n        \n        hits = sum(1 for token in query_tokens if token in search_text)\n        \n        if hits > 0:\n            start_line = element.get(\"start_line\", 0)\n            scored_matches.append((hits, start_line, element))\n\n    scored_matches.sort(key=lambda x: (-x[0], x[1]))\n\n    results = []\n    for _, _, element_data in scored_matches[:k]:\n        result_item = {\n            \"file\": element_data.get(\"file_path\"),\n            \"element_name\": element_data.get(\"element_name\"),\n            \"element_type\": element_data.get(\"element_type\"),\n            \"lines\": f\"{element_data.get('start_line', '?')}-{element_data.get('end_line', '?')}\",\n            \"docstring\": element_data.get(\"docstring\", \"\"),\n        }\n        if \"signature\" in element_data:\n            result_item[\"signature\"] = element_data[\"signature\"]\n        if \"source_code\" in element_data:\n            result_item[\"snippet\"] = element_data[\"source_code\"]\n        \n        results.append(result_item)\n        \n    return results\n"
  },
  {
    "file_path": "context_store_json.py",
    "element_name": "main_cli",
    "element_type": "FunctionDef",
    "start_line": 241,
    "end_line": 279,
    "docstring": "Runs the command-line interface for building or querying JSON indices.",
    "source_code": "def main_cli():\n    \"\"\"Runs the command-line interface for building or querying JSON indices.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Build or query lightweight JSON-based context indices for Python codebases.\"\n    )\n    subparsers = parser.add_subparsers(dest=\"command\", required=True,\n                                     help=\"Action to perform: 'build' or 'query'.\")\n\n    build_cmd_parser = subparsers.add_parser(\"build\",\n                                          help=\"Scan a Python repository and create JSON index files.\")\n    build_cmd_parser.add_argument(\"--repo\", type=str, required=True,\n                                  help=\"Path to the root directory of the Python repository.\")\n    build_cmd_parser.add_argument(\"--output-dir\", type=str, default=\".\",\n                                  help=\"Directory to save the generated JSON index files (e.g., my_repo_signatures.json). Defaults to current directory.\")\n\n    query_cmd_parser = subparsers.add_parser(\"query\",\n                                         help=\"Query a JSON index file for relevant code elements.\")\n    query_cmd_parser.add_argument(\"--index\", type=str, required=True,\n                                  help=\"Path to the JSON index file to query (either _signatures.json or _fullsource.json).\")\n    query_cmd_parser.add_argument(\"--query\", type=str, required=True,\n                                  help=\"Search query string (searches element names and docstrings).\")\n    query_cmd_parser.add_argument(\"--k\", type=int, default=3,\n                                  help=\"Number of top results to return (default: 3).\")\n\n    argv = sys.argv[1:]\n    if not argv and sys.stdin.isatty(): \n        parser.print_help(sys.stderr)\n        sys.exit(1)\n        \n    args = parser.parse_args(argv if argv else None)\n\n    if args.command == \"build\":\n        build_json_indices(args.repo, args.output_dir)\n    elif args.command == \"query\":\n        query_results = query_json_file(args.query, args.index, args.k)\n        if query_results:\n            print(json.dumps(query_results, ensure_ascii=False, indent=2))\n        else:\n            print(\"No matching results found.\", file=sys.stderr)\n"
  }
]